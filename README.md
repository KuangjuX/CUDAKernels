# CUDAKernels

## Introduction

The Project is used to collect the CUDA kernels that I have written by hand, for the purpose of learning various CUDA techniques and conducting performance evaluation.

## Kernels

- [Reduce Sum](src/kernels/reduce.cu): ä½¿ç”¨ Warp Reduce å®ç°çš„ reduce sum æ“ä½œ / Reduce Sum Operation with **Warp Reduce**.
- [Reduce Max](src/kernels/reduce.cu): ä½¿ç”¨ Warp Reduce å®ç°çš„ reduce max æ“ä½œ / Reduce Max Operation with **Warp Reduce**.
- [Softmax](src/kernels/softmax.cu): ä½¿ç”¨ Warp Reduce å®ç°çš„æœªåˆ†å—çš„ softmax æ“ä½œ / Softmax Operation with **Warp Reduce**.

## Notes

- [Vectorized Memory Access](notes/memory/vec.md): å‘é‡åŒ–å†…å­˜è®¿é—®ç¬”è®° / Notes about Vectorized Memory Access.
- [Memory Coalescing](notes/memory/coalescing.md): å†…å­˜åˆå¹¶è®¿é—®ç¬”è®° / Notes about Memory Coalescing.
- [Warp-Level Primitives](notes/warp.md): Warp åŸè¯­ç¬”è®° / Notes about Warp-Level Primitives.
- [FlashAttention](notes/flash_attn.md): FlashAttention ç¬”è®° / Notes about FlashAttention.

## References

- [CUDA-Learn-Notes: ğŸ‰CUDA ç¬”è®° / å¤§æ¨¡å‹æ‰‹æ’•CUDA / C++ç¬”è®°ï¼Œæ›´æ–°éšç¼˜: flash_attnã€sgemmã€sgemvã€warp reduceã€block reduceã€dot productã€elementwiseã€softmaxã€layernormã€rmsnormã€hist etc.](https://github.com/DefTruth/CUDA-Learn-Notes)
- [ThunderKittens: Tile primitives for speedy kernels](https://github.com/HazyResearch/ThunderKittens)