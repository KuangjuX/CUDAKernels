# CUDAKernels

## Introduction

The Project is used to collect the CUDA kernels that I have written by hand, for the purpose of learning various CUDA techniques and conducting performance evaluation.

## Abstractions

- [CUDA Vector Registers](include/memory/types/register.hpp): CUDA å‘é‡åŒ–å¯„å­˜å™¨æŠ½è±¡ / CUDA Vectorize Register Abstractions.

## Kernels

- [Reduce Sum](src/kernels/reduce.cu): ä½¿ç”¨ Warp Reduce å®ç°çš„ reduce sum æ“ä½œ / Reduce Sum Operation with **Warp Reduce**.
- [Reduce Max](src/kernels/reduce.cu): ä½¿ç”¨ Warp Reduce å®ç°çš„ reduce max æ“ä½œ / Reduce Max Operation with **Warp Reduce**.
- [Softmax](src/kernels/softmax.cu): ä½¿ç”¨ Warp Reduce å®ç°çš„æœªåˆ†å—çš„ softmax æ“ä½œ / Softmax Operation with **Warp Reduce**.
- [Vectorize Load/Store](src/kernels/memory/vec.cu): å‘é‡åŒ–åŠ è½½ä¸å­˜å‚¨ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ Global åˆ° Sharedï¼ŒShared åˆ° RF ä»¥åŠ Global åˆ° RF / Vectorize Load/Store Optimization.
- [FlashAttention](src/kernels/flash_attn/flash_attn_f32.cu): FlashAttention çš„ CUDA å®ç° / FlashAttention Implementation with CUDA.

## Notes

- [Vectorized Memory Access](notes/memory/vec.md): å‘é‡åŒ–å†…å­˜è®¿é—®ç¬”è®° / Notes about Vectorized Memory Access.
- [Memory Coalescing](notes/memory/coalescing.md): å†…å­˜åˆå¹¶è®¿é—®ç¬”è®° / Notes about Memory Coalescing.
- [Warp-Level Primitives](notes/warp.md): Warp åŸè¯­ç¬”è®° / Notes about Warp-Level Primitives.
- [FlashAttention](notes/flash_attn.md): FlashAttention ç¬”è®° / Notes about FlashAttention.

## References

- [CUDA-Learn-Notes: ğŸ‰CUDA ç¬”è®° / å¤§æ¨¡å‹æ‰‹æ’•CUDA / C++ç¬”è®°ï¼Œæ›´æ–°éšç¼˜: flash_attnã€sgemmã€sgemvã€warp reduceã€block reduceã€dot productã€elementwiseã€softmaxã€layernormã€rmsnormã€hist etc.](https://github.com/DefTruth/CUDA-Learn-Notes)
- [ThunderKittens: Tile primitives for speedy kernels](https://github.com/HazyResearch/ThunderKittens)